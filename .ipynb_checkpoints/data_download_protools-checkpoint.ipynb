{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d317d91e",
   "metadata": {},
   "source": [
    "<b>Import Data</b>\n",
    "\n",
    "Data Criteria:<br>\n",
    "(1) Minimum 5000 unique post for each sub-reddits<br>\n",
    "(2) Must be a text post<br>\n",
    "(3) Each post can't be too short (minimum 20 words)<br>\n",
    "(4) The selected subreddits can't be too different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204c057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8a98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.pushshift.io/reddit/search/submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0de1f8",
   "metadata": {},
   "source": [
    "<b>Define Initial Cleaning Process</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62eb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(line):\n",
    "    \n",
    "    line = str(line)\n",
    "    \n",
    "    line = line.replace('\\n', ' ') #remove nextline\n",
    "    \n",
    "    line = line.replace(\"’\", \"'\") #replace ’ as '\n",
    "    \n",
    "    line = re.sub(r'http\\S+', '', line) #remove URL\n",
    "    \n",
    "    line = line.replace(\"&amp;\", \"&\") #replace '&amp; as ampersand\n",
    "    \n",
    "    line = line.replace(\"#x200B;\", \" \") #replace '&amp; as space\n",
    "\n",
    "    if line == '[removed]': #replace [removed] as blank\n",
    "        line = np.nan\n",
    "        \n",
    "    if line == 'nan':\n",
    "        line = np.nan\n",
    "            \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55f7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intiate cut-off end date (17 Nov 2022, 23:59:59) 1668470399\n",
    "#when error occured while obtaining data, will update this timestamp to continue with the download\n",
    "timestamp = 1668470399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b30b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249    1665242256\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 1 Valid Rows: 179 Cummulated Rows: 179\n",
      "249    1661439214\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 2 Valid Rows: 182 Cummulated Rows: 361\n",
      "249    1656795624\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 3 Valid Rows: 187 Cummulated Rows: 548\n",
      "249    1652702976\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 4 Valid Rows: 173 Cummulated Rows: 721\n",
      "249    1649198636\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 5 Valid Rows: 179 Cummulated Rows: 900\n",
      "249    1645511660\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 6 Valid Rows: 189 Cummulated Rows: 1089\n",
      "249    1642203503\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 7 Valid Rows: 166 Cummulated Rows: 1255\n",
      "249    1638656090\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 8 Valid Rows: 172 Cummulated Rows: 1427\n",
      "249    1635551407\n",
      "Name: created_utc, dtype: int64\n",
      "Iternation: 9 Valid Rows: 161 Cummulated Rows: 1588\n"
     ]
    }
   ],
   "source": [
    "# initiate loop to get data for r/ableton in batches\n",
    "\n",
    "n = 1\n",
    "\n",
    "while int(timestamp) > 1636934399: #set to 1 year (17 Nov 2021, 23:59:59)\n",
    "\n",
    "    params = {'subreddit' : 'FL_Studio', 'size': 500, 'before': timestamp,} #subreddit code change accordingly\n",
    "    res = requests.get(url, params)\n",
    "    res.status_code\n",
    "    data = res.json()\n",
    "    df = pd.DataFrame(data['data'])\n",
    "    \n",
    "    timestamp = df[-1:]['created_utc']\n",
    "    \n",
    "    print(timestamp)\n",
    "    \n",
    "    df['selftext'] = df['selftext'].apply(cleaning)\n",
    "\n",
    "    df['wordcount'] = [len(re.findall(r'\\w+', str(i))) for i in df['selftext']]\n",
    "\n",
    "    df = df.drop(df[df.wordcount < 20].index)\n",
    "   \n",
    "    if n == 1:\n",
    "        df_main = df\n",
    "    else:\n",
    "        df_main = pd.concat([df_main, df])\n",
    "\n",
    "    print(\"Iternation: \" + str(n) + \n",
    "          \" Valid Rows: \" + str(df.shape[0]) + \n",
    "          \" Cummulated Rows: \" + str(df_main.shape[0]))\n",
    "        \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771ba247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Rows After Deduplication: 1537\n"
     ]
    }
   ],
   "source": [
    "df_main = df_main.drop_duplicates(subset='selftext', keep=\"first\")\n",
    "print(\"Final Rows After Deduplication: \" + str(df_main.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d881416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_csv(r'ableton_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce992fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
